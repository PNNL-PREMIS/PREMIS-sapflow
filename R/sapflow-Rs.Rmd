---
title: "sapflow-Rs"
author: "SP"
date: "9/10/2019"
output:
  html_document: default
---

**Title:** (From AGU) Using continuous sap flux and soil respiration datasets to infer the strength and speed of root-soil coupling in a deciduous forest

**Authors:** Stephanie C. Pennington, Ben Bond-Lamberty, Charlotte Grossiord, Wenzhi Wang, Nate McDowell

**Target Journal:**

**Overall Scientific Question:** What is strength and speed of above-belowground coupling (Js to Rs)?

### Data Prep and Preliminary Figures: 

```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```

```{r functions, include = FALSE, message = FALSE}
# Load functions
library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
theme_set(theme_minimal())
library(lubridate)
library(viridis)
library(wesanderson)
library(kableExtra)
library(suncalc)

# find matches
source(file = "find_day.R")

# count observations
obs_count_data <- tibble()
step_count <- 0
log_obs <- function(d, step_name) {
  step_count <<- step_count + 1
  d %>% 
    group_by(Tree) %>% 
    summarise(step = step_count,
              step_name = step_name,
              n = n(),
              .groups = "drop") %>% 
    bind_rows(obs_count_data) ->> obs_count_data
  d
}

# compute rs-js correlation
compute_lag_cor <- function(rs, js, hour_range) {
  # `rs` and `js` are numeric vectors with the same length
  # hour_range is a numeric vector of length 2 that contains the start and end hour to lag
  hours <- min(hour_range):max(hour_range)
  result <- tibble(Hour = hours, Cor = NA)
  
  if(sum(!is.na(rs)) < 6 | sum(!is.na(js)) < 6) {
    return(result)  # just NA
  }
  
  for(i in seq_along(hours)) {
    js_lag <- lead(js, n = hours[i])
    result$Cor[i] <- cor(rs, js_lag, use = "na.or.complete")
  }
  return(result)
}

```

```{r load-data, echo = FALSE, message = FALSE, warning = FALSE}
# Load in Rs, K, weather, and inventory data
inventory <- read_csv("../../PREMIS-stormsurge/inventory/ss-inventory.csv",
                      col_types = "ccddccccdcccc")
ports_lt <- read_csv("../../PREMIS-ghg/design/LT_portcodes.csv", col_types = "dc")
species_codes <- read_csv("../../PREMIS-ghg/inventory_data/species_codes.csv",
                          col_types = "ccc")

read_csv("../SERC/baseliner_data/control_Kest_yr.csv", 
         col_names = c("Year", "DOY","Time","VPD", "PAR", paste0("C", 1:8)),
         col_types = strrep("d", 13)) %>% 
  mutate(Date = as_date(DOY - 1, origin = "2018-01-01")) %>% 
  separate(Date, c("Y", "Month", "Day"), sep = "-") %>% 
  separate(Time, c("Hour", "Min"), sep = -2) -> control_hm

control_hm$Hour[control_hm$Hour == ""] <- 0

control_hm %>% 
  mutate(Timestamp = mdy_hm(paste(Month, Day, Year, Hour, Min), tz = "EST")) %>%
  select(-Y, -Year, - Day, - Month) %>% 
  gather("Tree", "K", C1:C8) %>% 
  log_obs("control_hm gather") %>% 
  left_join(inventory, by = c("Tree" = "Sapflux")) %>% 
  log_obs("join with inventory") %>% 
  select(Timestamp, PAR, K, Tree, Species_code, Plot) -> control_long

##check where licor data is coming from 
read_csv("../SERC/con_licor_data_20191120.csv",
         col_types = "dTcddddddddddc") %>%
  mutate(Timestamp = force_tz(Timestamp, tz = "EST")) %>% 
  left_join(ports_lt, by = "Port") %>% 
  left_join(inventory, by = c("Tree" = "Sapflux"))-> rs_long

# Load weather data
read_csv("../SERC/met_data/SECPRE_30min.csv",
         col_types = "ccddTTddddT") %>% 
  mutate(Timestamp = ymd_hms(endDateTime, tz = "UTC")) %>% 
  rename(Precip = secPrecipBulk) %>% 
  select(Timestamp, Precip) -> precip

read_csv("../SERC/met_data/RH_30min.csv", 
         col_types = paste0("ccccTT", strrep("d", 24), "T")) %>% 
  filter(horizontalPosition == "000", verticalPosition == "060") %>% 
  mutate(Timestamp = ymd_hms(endDateTime, tz = "UTC")) %>% 
  rename(RH = RHMean, Tair = tempRHMean) %>% 
  select(Timestamp, RH, Tair) -> temprh

read_csv("../SERC/met_data/PARPAR_30min.csv",
         col_types = paste0("ccccTT", strrep("d", 16), "T")) %>% 
  filter(verticalPosition == "050") %>% 
  mutate(Timestamp = ymd_hms(endDateTime, tz = "UTC")) %>% 
  select(Timestamp, PAR = PARMean) %>%
  left_join(precip, by = "Timestamp") %>% 
  left_join(temprh, by = "Timestamp") %>% 
  # At this point convert everything to EST for the rest of the analysis
  # Note this doesn't change the times themselves, just associated time zone
  mutate(Timestamp = with_tz(Timestamp, tzone = "EST"),
         Timestamp = round_date(Timestamp, unit = "hour")) %>% 
  group_by(Timestamp) %>% 
  summarise(PAR = mean(PAR), 
            Precip = mean(Precip), 
            RH = mean(RH),
            Tair = mean(Tair),
            .groups = "drop") %>% 
  mutate(DOY = yday(Timestamp), 
         Date = date(Timestamp),
         es = (6.11 *10^((7.5 * Tair)/(273.3 + Tair)))/10,
         VPD = ((100 - RH) * es)/100) %>% 
  select(-es) -> wx_dat_no_dpar

# Compute daytime PAR as a new column
sunlight_times <- getSunlightTimes(wx_dat_no_dpar$Date,
                                   lat = 38.9, lon = -77, tz = "EST", 
                                   keep = c("sunrise", "sunset"))
wx_dat_no_dpar$sunrise <- sunlight_times$sunrise
wx_dat_no_dpar$sunset <- sunlight_times$sunset
wx_dat_no_dpar$Daytime <- with(wx_dat_no_dpar, Timestamp >= sunrise, Timestamp <= sunset)

wx_dat_no_dpar %>% 
  filter(Daytime) %>%   # daytime only
  group_by(DOY) %>% 
  summarise(Daytime_PAR = mean(PAR, na.rm = TRUE),
            .groups = "drop") %>% 
  # we've computed mean *daytime* PAR; now merge back in
  right_join(wx_dat_no_dpar, by = "DOY") %>% 
  # compute daytime PAR groups - cloudy, medium, sunny
  group_by(month(Timestamp)) %>% 
  mutate(PAR_group = as.factor(ntile(Daytime_PAR, 3))) %>% 
  ungroup() %>% 
  select(-`month(Timestamp)`) ->  # no longer needed
  wx_dat
```

## Diagnostic plots for weather data

```{r weather-qaqc}
# Monthly averages by hour for each variable
wx_dat %>% 
  gather(var, value, PAR, Precip, RH, Tair) %>% 
  mutate(month = month(Timestamp),
         hour = hour(Timestamp)) %>% 
  group_by(month, hour, var) %>% 
  summarise(value = mean(value, na.rm = TRUE),
            .groups = "drop") %>%
  ggplot(aes(hour, value, color = as.factor(month), group = month)) + 
  labs(color = "Month", x = "Hour of Day", y = "Value") +
  geom_line() + facet_grid(var~., scales = "free")

# Data by month for each variable, colored by day/night
wx_dat %>% 
  gather(var, value, PAR, Precip, RH, Tair) %>% 
  mutate(month = month(Timestamp),
         hour = hour(Timestamp)) %>% 
  group_by(month, Daytime, hour, var) %>% 
  summarise(value = mean(value, na.rm = TRUE),
            .groups = "drop") %>%
  ggplot(aes(as.factor(month), value, color = Daytime)) + 
  scale_color_viridis(discrete = TRUE,
                      name = "Time of Day",
                      labels = c("Nighttime", "Daytime")) +
  labs(x = "Month", y = "Value") +
  geom_jitter() + facet_grid(var~., scales = "free")

# PAR versus PAR groups
ggplot(wx_dat, aes(DOY, PAR)) + 
  geom_point(alpha = 0.2, color = "grey60") + 
  geom_point(aes(y = Daytime_PAR, color = PAR_group))
```


```{r data-cleaning, echo = FALSE, message = FALSE}
# Next, we need to filter for the Js trees that also have Rs measurements
rs_trees <- unique(ports_lt$Tree)

control_long %>% 
  #Then, we need to transform K to Js (sapflux density) through a conversion
  mutate(Js = 119* 10^(-6) * K^(1.231)) %>% 
  filter(Tree %in% rs_trees) %>% 
  log_obs("filter for rs_trees") %>% 
  # Then, we need to round the timestamp to the nearest hour and take hourly averages
  mutate(Timestamp = round_date(Timestamp, unit = "hour")) %>% 
  # Now that all data are on a common timeframe, we can take the hourly mean of each dataset
  group_by(Timestamp, Tree, Species_code) %>% 
  summarise(Js_avg = mean(Js, na.rm = TRUE),
            .groups = "drop") %>% 
  log_obs("summarise control_long by hour") -> control_filter

####RESOLVE THIS 
rs_long %>%
  log_obs("initial rs_long") %>% 
  mutate(Timestamp = round_date(Timestamp, unit = "hour")) %>%
  group_by(Timestamp, Tree, Species_code) %>%
  summarise(rs_avg = mean(Flux), 
            T5 = mean(T5, na.rm = TRUE), 
            SM = mean(SMoist, na.rm = TRUE),
            .groups = "drop") %>% 
  log_obs("summarise rs_long by hour") %>% 
  # need to replace bad T5 and SM data - column flagging data 
  filter(rs_avg > 0, rs_avg < 25, SM > 0, SM < 1, T5 < 50) %>%
  log_obs("filtering rs_long") -> rs_filter

control_filter %>%
  log_obs("control_filter") %>% 
  left_join(rs_filter) %>% 
  log_obs("join with rs_filter") %>% 
  left_join(wx_dat, by = "Timestamp") %>% 
  log_obs("join with wx_dat") %>% 
  complete(Tree, Timestamp = seq(min(Timestamp), max(Timestamp), by = "hour")) %>% 
  log_obs("combined") -> combined

# At this point, `combined` holds rs, js, and weather data rounded to the nearest hour

combined %>% 
  mutate(Date = date(Timestamp)) %>% 
  group_by(Date, Tree, PAR_group) %>% 
  summarise_if(is.numeric, mean, na.rm = TRUE) %>% 
  ungroup() %>% 
  log_obs("summarise_if by date") %>% 
  complete(Tree, Date = seq(ymd(min(Date)), ymd(max(Date)), by = "day")) %>% 
  mutate(DOY = yday(Date)) %>%  # recalculate to get rid of fractions
  log_obs("daily") -> daily
```

## Hysteresis between temperature and Rs / Js

```{r hysteresis, echo = FALSE}
# That hysteresis plot Peishi showed was cool. Let's make one here
combined %>% 
  mutate(Month = month(Timestamp), 
         Hour = hour(Timestamp)) ->
  combined_hyst

# Compute mean T5, Rs, and Js by month so we can make nice labels
combined_hyst %>% 
  group_by(Tree, Month) %>% 
  summarise(T5 = mean(T5, na.rm = TRUE),
            SM = round(mean(SM, na.rm = TRUE), 2),
            rs_avg = mean(rs_avg, na.rm = TRUE),
            Js_avg = mean(Js_avg, na.rm = TRUE),
            .groups = "drop") ->
  hyst_labels

# Utility plotting function
library(ggrepel)
hyst_plot <- function(combined_hyst, depvar) {
  combined_hyst %>% 
    group_by(Tree, Month, Hour) %>% 
    summarise(T5 = mean(T5, na.rm = TRUE),
              SM = mean(SM, na.rm = TRUE),
              rs_avg = mean(rs_avg, na.rm = TRUE),
              Js_avg = mean(Js_avg, na.rm = TRUE),
              .groups = "drop") %>% 
    ggplot(aes_string("T5", depvar, color = "Hour", group = "Month")) + 
    geom_path(na.rm = TRUE) + 
    scale_color_gradient2(low = "#132B43", mid = "#56B1F7", high = "#132B43", midpoint = 12) +
    facet_wrap(~Tree) +
    geom_text_repel(data = hyst_labels, aes(label = substr(month.name[Month], 1, 3)), 
                    size = 2, color = "black",
                    nudge_y = 4,
                    segment.colour = NA)
}

hyst_plot(combined_hyst, "rs_avg")
hyst_plot(combined_hyst, "Js_avg")
```


## Observation counts check

```{r check-counts, echo = FALSE, include = FALSE}
obs_count_data %>% 
  filter(Tree %in% rs_trees) %>% 
  spread(Tree,n) %>% 
  kable("html") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

## Js versus Rs
Split by daytime and nighttime values

```{r rs-js plot, echo = FALSE, message = FALSE}
ggplot(combined, aes(x = Js_avg, rs_avg)) +
  geom_point(aes(color = Daytime), alpha = 0.5) +
  facet_wrap(~Daytime, scales = "free", ncol = 1) +
  geom_smooth(aes(group = Daytime), color = "black", method = "lm") +
  scale_color_viridis(discrete = TRUE,
                      name = "Time of Day",
                      labels = c("Nighttime", "Daytime")) +
  scale_fill_viridis(discrete = TRUE,
                     name = "Time of Day",
                     labels = c("Nighttime", "Daytime")) +
  labs(x = "Js (m/s)", y = "Rs (umol/m/s)") +
  theme(strip.text.x = element_blank())

```

## Raw data over time

```{r raw-data-over-time, echo = FALSE}
ggplot(data = control_long, aes(x = Timestamp, y = K)) + 
  geom_line() + 
  facet_wrap(~Tree, scales = "free", ncol = 2)

ggplot(data = rs_long, aes(x = Timestamp, y = Flux)) +
  geom_line() +
  facet_wrap(~Tree, scales = "free")
```

## PACF
The partial autocorrelation function gives the partial correlation (i.e. after controlling for other variables, or in this case, time lags) of a stationary time series with its own lagged values.

- Note: There's a sharp flip in the correlation of Js values from hour 1 to 2. Interesting...

```{r pacf, echo = FALSE}

# Average across chambers and trees for each timestamp
combined %>% 
  group_by(Timestamp) %>% 
  summarise(rs_avg = mean(rs_avg, na.rm = TRUE),
            Js_avg = mean(Js_avg, na.rm = TRUE),
            .groups = "drop") %>% 
  arrange(Timestamp) ->
  combined_pacf

# Prerequisite: the time series shouldn't have any gaps
timediffs <- diff(combined_pacf$Timestamp)
if(all(duplicated(timediffs)[-1])) {
  title <- ""
} else {
  title <- "*** WARNING *** 'combined' has gaps or\ninconsistent difftimes"
}

pacf_rs <- pacf(combined_pacf$rs_avg, na.action = na.pass, plot = FALSE)
pacf_js <- pacf(combined_pacf$Js_avg, na.action = na.pass, plot = FALSE)
pacf_rs <- tibble(which = "R[S]", lag = pacf_rs$lag[,,1], PACF = pacf_rs$acf[,,1])
pacf_js <- tibble(which = "J[S]", lag = pacf_js$lag[,,1], PACF = pacf_js$acf[,,1])
bind_rows(pacf_js, pacf_rs) %>% 
  ggplot(aes(lag, PACF, fill = which)) + geom_col(position = "dodge") +
  xlim(c(0, 23)) +
  scale_fill_discrete("") +
  ggtitle(title)
```

## Science Questions.

### Q1: For the overall dataset, how correlated are Js and Rs, at what time lags? 
(Between the fluxes; here and afterward, on a per-tree basis.)

- Whole dataset
- Compute lag correlation for each tree - each timestamp hour

#### H1.1. Hypothesis-time. 
Js and Rs will be correlated at some lag of (probably) multiple hours, because of the time it takes for sap to ascend; photosynthesis to occur; phloem to descend to roots; respiration to occur; and resulting CO2 to diffuse to soil surface .

#### H1.2 Hypothesis-species. 
We expect there to be differences in peak lag and correlation between the two species - Tulip Poplar and Red Maple - driven by path length difference and light availability.

```{r species-comparison, echo = FALSE}
combined %>%
  group_by(Tree) %>%
  do(compute_lag_cor(.$rs_avg, .$Js_avg, hour_range = c(0, 23))) -> q1

ggplot(q1, aes(x = Hour, y = Cor, color = Tree)) + geom_line()

q1 %>% 
  left_join(inventory, by = c("Tree" = "Sapflux")) %>% 
  select(Tree, Species_code, Hour, Cor) %>% 
  group_by(Tree) %>% 
  summarise(`Species` = Species_code[which.max(Cor)], 
            `Hour Lag` = Hour[which.max(Cor)], 
            `Maximum Correlation` = max(Cor),
            .groups = "drop") %>% 
  select(-`.groups`) %>% 
  kable("html") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

```

### Q2: How does this change over the course of the growing season? 

- For each week of year, calculate the correlation between Js and Rs for each hour lag (using function) and pull out max correlation lag
- Dendrometer data to show growth changes?

#### H2.1. Hypothesis. 
We expect to see changes in the strength and speed of coupling, probably because of seasonal changes in photosynthetic capacity and carbon allocation (e.g. reflected in stem diameter growth data).

- note: there's definitely an error going on at ~week 38 in C8 we need to investigate

```{r seasonal-changes, echo = FALSE}
growing_season <- c(4, 5, 6, 7, 8, 9, 10)  # April through October

combined %>% 
  mutate(Month = month(Timestamp), 
         Week = week(Timestamp)) %>% 
  filter(Month %in% growing_season) -> growing # filter for growing season

growing %>% 
  group_by(Week, Tree) %>% 
  do(compute_lag_cor(.$rs_avg, .$Js_avg, hour_range = c(0, 23))) %>% 
  ungroup() %>% 
  na.omit() -> cor_by_week

ggplot(data = cor_by_week, aes(x = as.numeric(Week), y = Hour, fill = Cor)) + 
  geom_tile() + 
  facet_wrap(~Tree) + 
#  geom_vline(xintercept = 30) + # not sure what this was meant for
  scale_fill_distiller(palette = "RdBu") +
  xlab("Week of Year") + ylab("Hour of Day")
```

### Q3: Is this correlation or causation? 

#### H3.1. Hypothesis. 
If H1.1 is correct, then days with more sunlight would have a stronger correlation.

To examine this issue, we look at matched days, i.e. that are in the same part of the growing season and have similar conditions EXCEPT for sunlight.

**Goal:** To compare Rs:Js correlation of sunny vs. cloudy days. We essentially are testing the importance of PAR on the relationship.

- Rs, Js, and climate variables parsed to same timescale
- Data separated into **sunny days** (top 1/3 _daytime_ PAR) and **cloudy days** (bottom 1/3 _daytime_ PAR)
- Just the matched days, how do the max-cor-lags differ between them?
- Tree, DOY, Match_doy, Max_Cor, Lag_max_cor, Coverage

```{r sunny-cloudy-setup, echo=FALSE, message=FALSE, warning=FALSE}
# Take full dataset and split into groups based on PAR
daily %>% 
  filter(!is.na(PAR_group)) %>% 
  mutate(Coverage = c("Cloudy", "Medium", "Sunny")[PAR_group]) -> full_sc

sunny_days <- filter(full_sc, Coverage == "Sunny") %>% pull(DOY) %>% unique() %>% na.omit()
cloudy_days <- filter(full_sc, Coverage == "Cloudy") %>% pull(DOY) %>% unique() %>% na.omit()

full_sc %>% 
  select(DOY, VPD, T5, SM, Precip, RH, Tair) %>% 
  group_by(DOY) %>% 
  summarise_all(mean, na.rm = TRUE) ->
  climate_data

# fsd - find similar days for sunny_days (numeric vector)
fsd <- function(sunny_days, climate_data, lookahead, constraints) {
  matches <- list()
  for(sd in sunny_days) {
    matches[[sd]] <- tibble(DOY = sd,
                            Similar_Day = similar_days(sd, climate_data, lookahead, constraints))
  }
  bind_rows(matches)
}
```

#### Constraint sensitivity test

We've created a function `similar_days` to match days of similar climate conditions but varying PAR (i.e. sunny-cloudy days)

First, we tested how the constraints for climate conditions impacted the number of matches returned

```{r sunny-cloudy-sensitivity, echo=FALSE, cache = TRUE}
constraints <- c("VPD" = 0.5, "T5" = 2, "SM" = 0.2, "RH" = 10, "Tair" = 2)

daycount <- function(constraints, lookahead = 8) {
  suppressMessages(
    nrow(fsd(sunny_days, climate_data, lookahead, constraints))
  )
}

la_seq <- 1:20
df1 <- tibble(variable = "lookahead",
              value = la_seq,
              daycount = sapply(la_seq, function(x) daycount(constraints, lookahead = x)))
vpd_seq <- seq(0.01, 1, length.out = 20)
df2 <- tibble(variable = "vpd",
              value = vpd_seq,
              daycount = sapply(vpd_seq, function(x) daycount(constraints = c("VPD" = x))))
t5_seq <- seq(0.1, 5, length.out = 20)
df3 <- tibble(variable = "T5",
              value = t5_seq,
              daycount = sapply(vpd_seq, function(x) daycount(constraints = c("T5" = x))))
tair_seq <- seq(1, 35, length.out = 20)
df4 <- tibble(variable = "Tair",
              value = tair_seq,
              daycount = sapply(vpd_seq, function(x) daycount(constraints = c("Tair" = x))))
sm_seq <- seq(0.05, 1, length.out = 20)
df5 <- tibble(variable = "SM",
              value = sm_seq,
              daycount = sapply(sm_seq, function(x) daycount(constraints = c("SM" = x))))
rh_seq <- seq(5, 50, length.out = 20)
df6 <- tibble(variable = "RH",
              value = rh_seq,
              daycount = sapply(sm_seq, function(x) daycount(constraints = c("RH" = x))))

bind_rows(df1, df2, df3, df4, df5, df6) %>% 
  ggplot(aes(value, daycount)) + 
  geom_line() +
  xlab("Constraint value") + ylab("Number of matched days") +
  facet_wrap(~variable, scales = "free")
```

```{r sunny-cloudy-compute, echo=FALSE, message=FALSE, warning=FALSE}

constraints %>% 
  kable("html", col.names = " ") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>% 
  add_header_above(c("Constraint", "Range"))


# So we have a problem--seems like we're getting duplicated data?
# Let's follow C3 day 140 through the following code and see where things are going wrong

# Find the days that are similar to the sunny days, and filter those to known cloudy days
fsd(sunny_days, climate_data, lookahead = 14, constraints) %>% 
  filter(Similar_Day %in% cloudy_days) %>% 
  rename(Sunny_DOY = DOY, Cloudy_DOY = Similar_Day) -> matched_DOY

left_join(matched_DOY, combined, by = c("Cloudy_DOY" = "DOY")) %>%
  mutate(Coverage = "Cloudy") %>% 
  rename(DOY = Cloudy_DOY, Group_No = Sunny_DOY) -> cloudy_complete

left_join(matched_DOY, combined, by = c("Sunny_DOY" = "DOY")) %>% 
  mutate(Coverage = "Sunny", Group_No = Sunny_DOY) %>% 
  rename(DOY = Sunny_DOY) %>% 
  select(-Cloudy_DOY) -> sunny_complete

bind_rows(cloudy_complete, sunny_complete) -> matches_data

# We're can only use sunny-day-groups with data in them. Compute this
matches_data %>% 
  # for each group and sunny/cloudy, how many observations?
  group_by(Group_No, Coverage) %>% 
  summarise(Js_N = sum(is.finite(Js_avg)),
            Rs_N = sum(is.finite(rs_avg)),
            .groups = "drop_last") %>%
  # which groups have sunny AND cloudy data available (six or more)?
  summarise(Js_suncloud_avail = all(Js_N > 5),
            Rs_suncloud_avail = all(Rs_N > 5),
            .groups = "drop") %>% 
  right_join(matches_data, by = "Group_No") %>% 
  filter(!is.na(Tree)) ->
  matches_data
```

```{r q3a, echo = FALSE, include = FALSE}
# Lag over whole growing season by sunny vs. cloudy
# should be using DAILY PAR FOR THIS!!!!!!!!!!!!
full_sc %>%
  group_by(Tree, Coverage) %>% na.omit() %>% 
  do(compute_lag_cor(.$rs_avg, .$Js_avg, hour_range = c(0, 23))) -> q3a

ggplot(data = q3a, aes(x = Hour, y = Cor, color = Coverage, group = Coverage)) + 
  geom_line() +
  facet_wrap(~Tree) +
  ylab("Correlation") + xlab("Hour of Day")

```

## Sunny-cloudy comparison

```{r sunny-cloudy-differences, echo = FALSE, message = FALSE}
# Boxplots: sunny versus cloudy days
ggplot(matches_data, aes(Tree, Js_avg, color = Coverage)) + 
  geom_boxplot() + 
  facet_wrap(~Tree, scales = "free")

# Lags, sunny days versus cloudy
# matches_data %>% 
#   arrange(Timestamp) %>% 
#   group_by(Tree, Coverage, DOY) %>% 
#   do(compute_lag_cor(.$rs_avg, .$Js_avg, hour_range = c(0, 13))) %>% 
#   ggplot(aes(Hour, Cor, color = Coverage, group = paste(Coverage, DOY))) + 
#   geom_line() + 
#   facet_wrap(~Tree) +
#   ggtitle("Each line is a day")
```

```{r sunny-cloudy-doy, echo = FALSE, message = FALSE, fig.height = 15}
# Plot of sunny DOYs with cloudy DOYs

matches_data %>% 
  filter(Js_suncloud_avail, Species_code == "LITU") %>% 
  na.omit() %>% 
  ggplot(aes(x = hour(Timestamp), y = Js_avg, color = Coverage)) +
  geom_point(aes(group = paste(DOY, Coverage, Tree))) + 
  geom_line(aes(group = paste(DOY, Coverage, Tree))) + 
  facet_wrap(~Group_No, scales = "free", ncol = 3)

matches_data %>% 
  filter(Rs_suncloud_avail, Species_code == "LITU") %>% 
  mutate(groups = paste(DOY, Coverage, Tree)) %>% 
  na.omit() %>% 
  ggplot(aes(x = hour(Timestamp), y = rs_avg, color = Coverage)) +
  geom_point(aes(group = groups)) + 
  geom_line(aes(group = groups)) +
  facet_wrap(~Group_No, scales = "free", ncol = 3)
```

## Sunny-cloudy Q10

```{r sunny-cloudy-q10, echo = FALSE, message = FALSE}

## Sunny Q10
sunny <- filter(matches_data, Coverage == "Sunny", !is.na(T5), !is.na(rs_avg))
model_s <- lm(log(rs_avg) ~ T5, data = sunny)
sunny$prediction <- exp(predict(model_s))
# ggplot(daytime, aes(x = T5, y = rs_avg)) + 
#   geom_point() +
#   geom_line(data = daytime, aes(y = prediction), linetype = 2)
# cat("Model Q10 =", exp(10 ^ model_d$coefficients[2]))

## Cloudy Q10
cloudy <- filter(matches_data, Coverage == "Cloudy", !is.na(T5), !is.na(rs_avg))
model_c <- lm(log(rs_avg) ~ T5, data = cloudy)
cloudy$prediction <- exp(predict(model_c))
# ggplot(nighttime, aes(x = T5, y = rs_avg)) + 
#   geom_point() + 
#   geom_line(data = nighttime, aes(y = prediction), linetype = 2)
# cat("Model Q10 =", exp(10 ^ model_n$coefficients[2]))

## Plot
ggplot(matches_data, aes(x = T5, y = rs_avg, color = Coverage)) + 
  geom_point(alpha = 0.3) +
  scale_color_viridis(discrete = TRUE) +
  ylab("Rs") +
  annotate("text", x = 10, y = 21.5, 
           label = paste("Q10 =", round(exp(10 ^ model_s$coefficients[2]), digits = 3)), 
           color = "#FDE725FF") +
  annotate("text", x = 10, y = 20, 
           label = paste("Q10=", round(exp(10 ^ model_c$coefficients[2]), digits = 3)), 
           color = "#440154FF")

```
